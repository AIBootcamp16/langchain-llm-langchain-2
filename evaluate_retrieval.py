"""
Retrieval ì •í™•ë„ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- eval_set.json ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ Hit Rate ì¸¡ì •
- ì •ë‹µ(ground_truth)ì— í¬í•¨ëœ ë²•ë ¹ ì¡°í•­ì´ ê²€ìƒ‰ ê²°ê³¼ì— ìˆëŠ”ì§€ í™•ì¸
- ë²¡í„° ê²€ìƒ‰, BM25, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë¹„êµ ê°€ëŠ¥
"""

import json
import re
import sys
from pathlib import Path
from typing import List, Dict, Optional, Any, Callable
from dataclasses import dataclass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent / "src"))

from vectorstore import VectorStore
from retriever import HybridRetriever, KiwiBM25Retriever


@dataclass
class EvalSample:
    """í‰ê°€ ìƒ˜í”Œ"""
    question: str
    ground_truth: str
    target_law: str  # ë²•ë ¹ëª…+ì¡°í•­ (ì˜ˆ: "í˜•ë²•ì œ17ì¡°")
    target_article: str  # ì¡°í•­ë§Œ (ì˜ˆ: "17ì¡°")
    doc_type: str
    doc_id: str


def load_eval_set(eval_set_path: str = "eval_set_retrieval.json") -> List[EvalSample]:
    """
    eval_set_retrieval.json íŒŒì¼ì„ ë¡œë“œí•˜ê³  í‰ê°€ ìƒ˜í”Œ ìƒì„±
    (law_name, article í•„ë“œê°€ ì´ë¯¸ ì¶”ì¶œë˜ì–´ ìˆëŠ” ë°ì´í„°ì…‹ ì‚¬ìš©)

    Args:
        eval_set_path: eval_set_retrieval.json íŒŒì¼ ê²½ë¡œ
    """
    print(f"ğŸ“‚ í‰ê°€ ë°ì´í„° ë¡œë“œ ì¤‘: {eval_set_path}")

    with open(eval_set_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    items = data.get("items", [])
    samples = []
    skipped = 0

    for item in items:
        question = item.get("question", "")
        ground_truth = item.get("ground_truth", "")
        doc_type = item.get("doc_type", "")
        doc_id = item.get("doc_id", "")
        law_name = item.get("law_name", "")
        article = item.get("article", "")

        if not question or not law_name or not article:
            skipped += 1
            continue

        # law_name + ì œ + article í˜•íƒœë¡œ target_law ìƒì„±
        target_law = f"{law_name}ì œ{article}"
        target_article = f"ì œ{article}"  # ì¡°í•­ë§Œ (ì˜ˆ: "ì œ17ì¡°")

        samples.append(EvalSample(
            question=question,
            ground_truth=ground_truth,
            target_law=target_law,
            target_article=target_article,
            doc_type=doc_type,
            doc_id=doc_id
        ))

    print(f"  ì´ {len(items)}ê°œ ì¤‘ {len(samples)}ê°œ í‰ê°€ ê°€ëŠ¥ (ìŠ¤í‚µ: {skipped}ê°œ)")
    return samples


def run_evaluation(
    search_fn: Callable[[str, int], List[Dict[str, Any]]],
    samples: List[EvalSample],
    top_k: int = 5,
    verbose: bool = True,
    mode_name: str = "Vector"
) -> Dict:
    """
    Retrieval í‰ê°€ ì‹¤í–‰

    Args:
        search_fn: ê²€ìƒ‰ í•¨ìˆ˜ (query, top_k) -> results
        samples: í‰ê°€ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
        top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜
        verbose: ì˜¤ë‹µ ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        mode_name: ê²€ìƒ‰ ëª¨ë“œ ì´ë¦„ (ì¶œë ¥ìš©)
    """
    print(f"\n{'='*50}")
    print(f"ğŸš€ Retrieval í‰ê°€ ì‹œì‘ [{mode_name}] (top_k={top_k})")
    print(f"{'='*50}\n")

    total_count = len(samples)
    correct_count = 0
    errors = []

    # doc_typeë³„ í†µê³„
    type_stats = {}

    for i, sample in enumerate(samples):
        # ê²€ìƒ‰ ìˆ˜í–‰
        try:
            results = search_fn(sample.question, top_k)
        except Exception as e:
            print(f"âŒ [ERROR] ë¬¸ì œ {i+1} ê²€ìƒ‰ ì¤‘ ì—ëŸ¬: {e}")
            continue

        # Hit ì—¬ë¶€ í™•ì¸
        # ChromaDB ì²­í¬ì—ëŠ” ë²•ë ¹ëª…ì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°í•­ ë²ˆí˜¸ë¡œ ë§¤ì¹­
        is_hit = False
        hit_doc_idx = -1
        retrieved_contents = []

        for idx, doc in enumerate(results):
            content = doc.get("content", "")

            # contentì—ì„œ "ì œNì¡°(ì œëª©)" íŒ¨í„´ ì¶”ì¶œ
            article_match = re.search(r"(ì œ\d+ì¡°(?:ì˜\d+)?)\s*\(([^)]+)\)", content)
            if article_match:
                article_info = f"{article_match.group(1)}({article_match.group(2)})"
            else:
                # ì œNì¡°ë§Œì´ë¼ë„ ì¶”ì¶œ
                simple_match = re.search(r"(ì œ\d+ì¡°(?:ì˜\d+)?)", content)
                article_info = simple_match.group(1) if simple_match else "ì¡°í•­ì—†ìŒ"

            # ì¡°í•­ ì •ë³´ + ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
            preview = f"[{article_info}] {content[:50]}..."
            retrieved_contents.append(preview)

            # ì—„ê²© ë§¤ì¹­: "ì œNì¡°(ì œëª©)" ë˜ëŠ” "ì œNì¡° â‘ " í˜•íƒœë¡œ ì¡°í•­ì´ ì‹œì‘í•˜ëŠ” ê²½ìš°ë§Œ ì¸ì •
            # ë‹¨ìˆœíˆ "ì œ4ì¡°ì— ë”°ë¼..." ê°™ì€ ì°¸ì¡°ëŠ” ì œì™¸
            strict_pattern = rf"{re.escape(sample.target_article)}\s*[\(â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]"
            if not is_hit and re.search(strict_pattern, content):
                is_hit = True
                hit_doc_idx = idx

        # doc_typeë³„ í†µê³„ ì—…ë°ì´íŠ¸
        if sample.doc_type not in type_stats:
            type_stats[sample.doc_type] = {"total": 0, "correct": 0}
        type_stats[sample.doc_type]["total"] += 1

        if is_hit:
            correct_count += 1
            type_stats[sample.doc_type]["correct"] += 1
            if verbose:
                print(f"âœ… [ì •ë‹µ] ë¬¸ì œ {i+1} ({sample.doc_type})")
                print(f"   - ì§ˆë¬¸: {sample.question[:80]}...")
                print(f"   - ëª©í‘œ: {sample.target_law} (ë§¤ì¹­: {sample.target_article})")
                print(f"   - ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:")
                for j, content in enumerate(retrieved_contents):
                    marker = "â­" if j == hit_doc_idx else "  "
                    print(f"     {marker}[{j+1}] {content}")
                print("-" * 50)
        else:
            errors.append({
                "index": i + 1,
                "question": sample.question,
                "target": sample.target_law,
                "doc_type": sample.doc_type,
                "retrieved": retrieved_contents
            })

            if verbose and len(errors) <= 10:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
                print(f"âŒ [ì˜¤ë‹µ] ë¬¸ì œ {i+1} ({sample.doc_type})")
                print(f"   - ì§ˆë¬¸: {sample.question[:80]}...")
                print(f"   - ëª©í‘œ: {sample.target_law} (ë§¤ì¹­: {sample.target_article})")
                print(f"   - ê²€ìƒ‰ëœ ê²ƒ: {retrieved_contents[0] if retrieved_contents else 'None'}")
                print("-" * 50)

    # ê²°ê³¼ ê³„ì‚°
    accuracy = (correct_count / total_count) * 100 if total_count > 0 else 0

    # ìµœì¢… ë¦¬í¬íŠ¸
    print(f"\n{'='*50}")
    print(f"ğŸ“Š [ìµœì¢… ê²°ê³¼ ë¦¬í¬íŠ¸] - {mode_name}")
    print(f"{'='*50}")
    print(f"ì´ í‰ê°€ ë¬¸ì œ ìˆ˜: {total_count}ê°œ")
    print(f"âœ… ì •ë‹µ (Hit): {correct_count}ê°œ")
    print(f"âŒ ì˜¤ë‹µ (Miss): {total_count - correct_count}ê°œ")
    print(f"ğŸ† Hit Rate @ {top_k}: {accuracy:.2f}%")

    # doc_typeë³„ ê²°ê³¼
    print(f"\nğŸ“ˆ [ìœ í˜•ë³„ Hit Rate]")
    for doc_type, stats in sorted(type_stats.items()):
        type_acc = (stats["correct"] / stats["total"]) * 100 if stats["total"] > 0 else 0
        print(f"  {doc_type}: {stats['correct']}/{stats['total']} ({type_acc:.1f}%)")

    print(f"{'='*50}")

    return {
        "mode": mode_name,
        "total": total_count,
        "correct": correct_count,
        "errors": len(errors),
        "hit_rate": accuracy,
        "top_k": top_k,
        "type_stats": type_stats,
        "error_details": errors[:20]  # ì²˜ìŒ 20ê°œ ì˜¤ë‹µë§Œ ì €ì¥
    }


def save_results(results: Dict, output_path: str = "retrieval_eval_results.json"):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nê²°ê³¼ ì €ì¥ë¨: {output_path}")


def load_bm25_documents(vectorstore: VectorStore) -> List[Dict[str, Any]]:
    """ChromaDBì—ì„œ ëª¨ë“  ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ BM25 ì¸ë±ì‹±ìš©ìœ¼ë¡œ ë°˜í™˜"""
    print("ğŸ“š BM25 ì¸ë±ì‹±ì„ ìœ„í•´ ChromaDBì—ì„œ ë¬¸ì„œ ë¡œë“œ ì¤‘...")

    # ChromaDBì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    all_docs = vectorstore.collection.get(include=["documents", "metadatas"])

    documents = []
    for i, (doc, meta) in enumerate(zip(all_docs["documents"], all_docs["metadatas"])):
        documents.append({
            "content": doc,
            "metadata": meta
        })

    print(f"  {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    return documents


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Retrieval í‰ê°€")
    parser.add_argument("--eval-set", type=str, default="eval_set_retrieval.json",
                        help="í‰ê°€ ë°ì´í„° ê²½ë¡œ (ê¸°ë³¸: eval_set_retrieval.json)")
    parser.add_argument("--top-k", type=int, default=3,
                        help="ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜")
    parser.add_argument("--quiet", action="store_true",
                        help="ì˜¤ë‹µ ìƒì„¸ ì¶œë ¥ ì•ˆí•¨")
    parser.add_argument("--save", action="store_true",
                        help="ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥")
    parser.add_argument("--mode", type=str, default="vector",
                        choices=["vector", "bm25", "hybrid", "all"],
                        help="ê²€ìƒ‰ ëª¨ë“œ: vector, bm25, hybrid, all (ê¸°ë³¸: vector)")
    parser.add_argument("--no-reranker", action="store_true",
                        help="í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì—ì„œ ë¦¬ë­ì»¤ ë¹„í™œì„±í™”")
    args = parser.parse_args()

    samples = load_eval_set(args.eval_set)

    if not samples:
        print("í‰ê°€í•  ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    print("\nğŸ”§ VectorStore ì´ˆê¸°í™” ì¤‘...")
    vs = VectorStore()

    all_results = {}

    # ë²¡í„° ê²€ìƒ‰
    if args.mode in ["vector", "all"]:
        def vector_search(query: str, top_k: int) -> List[Dict]:
            return vs.search(query, n_results=top_k)

        results = run_evaluation(
            search_fn=vector_search,
            samples=samples,
            top_k=args.top_k,
            verbose=not args.quiet,
            mode_name="Vector"
        )
        all_results["vector"] = results

    # BM25 ë˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œ ë¬¸ì„œ ë¡œë“œ
    if args.mode in ["bm25", "hybrid", "all"]:
        documents = load_bm25_documents(vs)

    # BM25 ê²€ìƒ‰
    if args.mode in ["bm25", "all"]:
        print("\nğŸ”§ BM25 ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        bm25_retriever = KiwiBM25Retriever(documents)

        def bm25_search(query: str, top_k: int) -> List[Dict]:
            return bm25_retriever.search(query, top_k=top_k)

        results = run_evaluation(
            search_fn=bm25_search,
            samples=samples,
            top_k=args.top_k,
            verbose=not args.quiet,
            mode_name="BM25"
        )
        all_results["bm25"] = results

    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    if args.mode in ["hybrid", "all"]:
        print("\nğŸ”§ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        hybrid_retriever = HybridRetriever(
            vectorstore=vs,
            documents=documents,
            use_reranker=not args.no_reranker
        )

        def hybrid_search(query: str, top_k: int) -> List[Dict]:
            return hybrid_retriever.search(
                query,
                top_k=top_k,
                use_reranker=not args.no_reranker
            )

        mode_name = "Hybrid" if not args.no_reranker else "Hybrid (no reranker)"
        results = run_evaluation(
            search_fn=hybrid_search,
            samples=samples,
            top_k=args.top_k,
            verbose=not args.quiet,
            mode_name=mode_name
        )
        all_results["hybrid"] = results

    # ëª¨ë“  ëª¨ë“œ ë¹„êµ ìš”ì•½
    if args.mode == "all":
        print(f"\n{'='*60}")
        print("ğŸ“Š [ê²€ìƒ‰ ëª¨ë“œ ë¹„êµ ìš”ì•½]")
        print(f"{'='*60}")
        print(f"{'ëª¨ë“œ':<20} {'Hit Rate':<15} {'ì •ë‹µ/ì „ì²´':<15}")
        print("-" * 60)
        for mode, result in all_results.items():
            print(f"{result['mode']:<20} {result['hit_rate']:.2f}%{'':<8} {result['correct']}/{result['total']}")
        print(f"{'='*60}")

    # ê²°ê³¼ ì €ì¥
    if args.save:
        if args.mode == "all":
            save_results(all_results, "retrieval_eval_results_all.json")
        else:
            save_results(all_results.get(args.mode, {}))
